# 2025-11-06 ECMA-402 Meeting

## Logistics

### Attendees

- Shane Carr - Google i18n (SFC), Co-Moderator
- Ben Allen - Igalia (BAN)
- Jesse Alama - Igalia (JMN)
- Henri Sivonen - Mozilla (HJS)
- Daniel Minor - Mozilla (DLM)
- Richard Gibson - OpenJS Foundation (RGN)
- Frank Yung-Fong Tang - Google i18n, V8 (FYT)
- Yusuke Suzuki - Apple (YSZ)
- Kaleb Pace - Invited Expert

### Standing items

- [Discussion Board](https://github.com/tc39/ecma402/projects/2)
- [Status Wiki](https://github.com/tc39/ecma402/wiki/Proposal-and-PR-Progress-Tracking) -- please update!
- [Abbreviations](https://github.com/tc39/notes/blob/master/delegates.txt)
- [MDN Tracking](https://github.com/tc39/ecma402-mdn)
- [Meeting Calendar](https://calendar.google.com/calendar/embed?src=unicode.org_nubvqveeeol570uuu7kri513vc%40group.calendar.google.com)
- [Matrix](https://matrix.to/#/#tc39-ecma402:matrix.org)

## Status Updates

### Updates from the Editors

BAN: I've been focused on Intl Era Month Code so I don't have updates.

RGN: No updates.

### Updates from the MessageFormat Working Group

SFC: I'm not aware of any updates at this time.

### Updates from Implementers

https://github.com/tc39/ecma402/wiki/Proposal-and-PR-Progress-Tracking

FYT: Updates to proposals page for PRs that merged. 929, scheduled to release in v8 in 143, early December. 989, notation for PluralRules, and the compactDisplay for PluralRules. The variants, currently, 960, the variants, firefox already shipped, that is currently under the flag, we are trying to push it to 144. I do need some help from Apple, asking what WebKit’s position is, it would be nice if anyone working on JSC could have a public position on these PRs. 

YSZ: Already included in Safari, let me check the version, 

FYT: We’re having problems finding the bug, I don’t know why, when I search I just can’t find it. 

YSZ: It’s already shipped, it’s included in Safari 26, this year’s release.

SFC: It would be useful to make sure the JSC column is updated correctly on the wiki.

DLM: No updates from my end. 

### Updates from the W3C i18n Group

BAN: No updates. I believe EAO was going to attend the meetings.

SFC: I believe EAO isn't joining because of travel to Japan.

## Proposals and Discussion Topics

https://github.com/tc39/ecma402/projects/2

### Normative: In PluralRules, set compactDisplay only if notation is "compact" #1032

https://github.com/tc39/ecma402/pull/1032

FYT: Let’s start by looking at current ECMA-402 on NumberFormat and PluralRules. Currently in PluralRules we read a notation and then read a compactDisplay, then unconditionally set the compactDisplay to the value read. The internal slot for CompactDisplay will be short or long, but nothing else. In NumberFormat, we unconditionally read the compactDisplay, but only set it if we are using compact notation. If the notation is not compact there is no meaningful compactDisplay, which makes sense. So we could have “short”, “long”, or *undefined*. My proposal is to change the PluralRules one to match, leaving the internal slot undefined when not using “compact” display. In v8 whenever notation is not compact, the definition of a compactDisplay – it’s not easy to figure out what it really means. So we want to align with NumberFormat – either “long”, “short”,or *undefined*. Note that for PluralRules resolvedOptions will list compactDisplay as “short”, even when not using compact. The one question is whether we should even read it, but I don’t think I want to make it different from NumberFormat – there is a question of whether we should even read the compactDisplay option when not using compact, but I think we should align with what NumberFormat is doing. 

SFC: This seems like a good change to make – we don’t need to set the slot when notation is not compact. The impact is because of resolvedOptions – that’s the main reason this matters. The point in the spec where this is a normative requirement is that the spec as currently written requires us include this in resolvedOptions, and not having it in resolvedOptions motivates this change.

DLM: I endorse this change, and I’ve talked with anba and he endorses it as well.

FYT: I’ll ask for consensus to move it to TG1. 

SFC: YSZ?

YSZ: To me, it makes sense – I support this proposal. 

FYT: Question related to this: we’re going to put this on the agenda as a needs-consensus PR.

SFC: My other comment is that I think this is the third PR on this topic, I don’t know if there’s any learnings or postmortems we can take from this in terms of catching these issues before they land, but I would call on everyone on the call to take a deeper look at them to try to find these types of issues before tehy land. First it was a pull request adding the notation option, then compact display, and then a third one related to compactDisplay. I think the main learning here is that implementers as well as everyone on this call has a responsibility to detect these earlier – these would ideally have been resolved in one PR. The longer I’m on this committee the more I think we should favor proposals over PRs, in order to give us more time to look at these issues, but just because it’s a PR doesn’t mean we shouldn’t be careful in our process on reviewing it and landing it.

FYT: I have a hard time catching it until I try to run with the test, that is not clearly connected. I found this one when I tried to implement it and resolveOptions (something?). I think in the future we could always have tests written and prototyped before we try to land it. Consensus, but not landed until we have a test ready and have an implementation trying it. 

#### Conclusion

TG2 approves. Bring to the TG1 meeting.

### Fix corner-case issues identified in testing #10

https://github.com/tc39/proposal-intl-keep-trailing-zeros/pull/10

SFC: I’ll read out EAO’s comment, since he’s not here. He says:

I validated the proposed spec text with a patched fork of FormatJS, and this identified a few places where the spec text needs to be updated:

Leading zeros need to be discarded, so we count '0012.3' to have three string digits. To do so in the syntax-directed operation, I introduce ZeroDigits as a new syntax rule.

An elided leading zero needs to be accounted for, so '.45' should count as having three string digits.

Changes in exponents that reduce the number of leading zeros need to also reduce the string digit count accordingly. This means that when formatted as a percentage, '0.06' should format as if it had only one string digit, rather than three. This adjustment needs to be done potentially twice, as style: 'percent' can be combined with notation: 'engineering' or notation: 'scientific'.

SFC: Based on EAO’s description, this looks like a good change to bring. I am a reviewer on this, so I’ll need to review it to make sure it is right, but I do think that this is a good thing to bring to TG1. Are there any comments/questions on this PR, or the keep-trailing-zero proposal? I don’t think anyone has implemented this yet, because it’s 2.7, but I assume once it gets to stage 3 then FYT, YSK, and DMN will have more opinions on this.  If not, we can move to the next keep-trailing-zeroes topic, which is the more interesting one. 

### Trailing zeros are retained also after rounding long input without trailing zeros #11

https://github.com/tc39/proposal-intl-keep-trailing-zeros/issues/11

SFC: This is something we noticed while doing the debugging of the other issue:

The proposal as it currently stands does not only keep trailing zeros in the input value, but also (when formatting string inputs) keeps trailing zeros in the formatted output. Most of the time those are synonymous, but the latter occurs when a number is rounded so that it ends with trailing zeros:

```
const nf = new Intl.NumberFormat('en', { maximumFractionDigits: 1 });

nf.format('2.00') // now: '2', proposed: '2.0'
nf.format('2.03') // now: '2', proposed: '2.0'
nf.format('1.96') // now: '2', proposed: '2.0'

nf.format(2n) // now: '2', proposed: '2'
nf.format(2.03) // now: '2', proposed: '2'

nf.format(1.96) // now: '2', proposed: '2'
```

This behaviour is probably the right thing, and is internally consistent, but it should be identified as a slightly greater change to existing behaviour than has been previously noted.
The difference in behaviour will occur when an input string value has more fraction digits or significant digits than specified for the output, and the result of rounding is a value with fewer digits than specified for the output. The default value for maximumFractionDigits is 3.
SFC: The conclusion we reached in the Numerics call is that we believe this is the correct behaviour, but it’s correct behaviour with a broader scope than we initially realized. We realized that adding the option value to get the old behaviour back is more needed than we previously realized, but before we did that we are seeking input from this group re: whether this behaviour is what we feel is the appropriate/correct behaviour. This is long-standing behaviour and we have agreed that since this is a 2.7 proposal, but given this increased impact, is this something we still consider in-scope for the proposal, and something that we can move forward with?

HJS: Old in terms of old version of proposal, or old in terms of shipping in browsers today?

SFC: The proposed behaviour is what’s in the spec text that went to 2.7, but it’s not a behaviour that was noted before.

HJS: If we have an option, one way the option can go is the old behaviour, one is the new behaviour, why do we need to opt in to the old behaviour rather than opting into the new one?

SFC: Initially the proposal has been to just change the behaviour to the new behaviour, since we see the new behaviour is correct and the old behaviour incorrect. The question is whether an option to go back to the old behaviour needs to exist. 

SFC: Let’s move to the other PR for trailing-zeroes 

### Add trailingZeroDisplay: 'stripToMinimum' option value #12

https://github.com/tc39/proposal-intl-keep-trailing-zeros/pull/12

SFC: Currently the trailing zero display has two settings. Let me pull up the docs. Right now Intl.NumberFormat has a setting called trailingZeroDisplay, which has two options: “auto” and “stripIfInteger”. The link is below.

https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Intl/NumberFormat/NumberFormat#trailingzerodisplay

SFC: The option name that EAO proposed was stripToMinimum, the auto option is updated to be what the current proposal is – that’s what the proposal currently say. The setting stripToMinimum would say that we use this behaviour for floats, BigInts, and Strings, rather than this special String behaviour. 

SFC: The docs on MDN need to be updated, the normative statement re: trailingZeroDisplay is no longer accurate.

RGN: It’s a little inaccurate now, and will be more inaccurate. 

SFC: I’ll try to give an opinion to move the conversation along. So, initially I had been saying that I didn’t think the option was motivated, because you can get the current behavior by using a float or BigInt input rather than a String input, and this is a small enough case that it doesn’t seem motivated to be an option. I still think this is largely the case, but there are a few instances where there will be things you can do now that you wouldn’t be able to do without this option, and the main example of that is when you’re trying to format a decimal string with more sig digits than fit in a float, and you want to have the current behavior, that is not a thing you’d be able to do without having an option available. You could do it with manual rounding, you could do it with the Amount proposal, which has more options with controlling sig digits. I tend to think that this option is more motivated than it was before, but as a TG2 delegate I don’t know if it’s motivated enough for us to add. So my updated opinion is that I don’t think it’s motivated enough. RGN said at the numerics call that there are going to be delegates at plenary who are going to want the old behaviour, but I think we should wait for them to speak up rather than being proactive in the absence of a specific use case. 

HJS: Is there any webcompat analysis of this change? If there is an option, then opting into new behavior would not have webcompat risk, and if there isn’t an option it’s like, we think this is so important that we accept webcompat risk. But if there is an option, having it be this way around is not the way around that has previously been successful on the web, so why this way around? It seems like a path-dependent thing – first there was the idea to make the change unconditionally, and then to have the option.

SFC: My understanding is that EAO’s position has been that this does not carry a significant webcompat risk because we have previously changed the semantics of how string formatting works in Intl.NumberFormat, and we have evidence that that didn’t break the web. That’s been EAO’s position here, and this body has largely agreed with that assessment. The reason we’re adding this option is because this is an option that EAO had originally proposed, this group and plenary both didn’t seem to think it was motivated, but based on this new observation that numbers such as the ones in the issue, that were not explicitly noted before, also have behavior changes, does that mean that the option to get back the old behaviour more motivated? If we feel as a committee that this is not a webcompat problem, so unless that opinion changes then that doesn’t give motivation for the flag unless it’s motivated by use cases. If this group thinks there is a webcompat problem, we should not only add the option and also revisit what we want to make the default behavior. Does that make sense?

HJS: Yes. 

SFC: I think maybe folks here want a little more time to ruminate on this, and I believe that EAO does have a topic on the plenary agenda in Tokyo in order to bring this to a larger group. I’m going – based on the quietness of this room I’m going to say that TG2 is happy with the recommendation of the champions group, but doesn’t have a lot of additional opinions on the matter. We can see if plenary has thoughts, and bring that feedback back here.

### Stage 3 requirements #9

https://github.com/tc39/proposal-intl-keep-trailing-zeros/issues/9

SFC: It appears that we are basically ready for stage 3, but we’re not pushing for stage 3 at this meeting in order to get clarity on the question we just asked, and then plan to get stage 3 at January’s meeting. Thoughts or comments? We will have at least one more meeting before we formally approve it for stage 3, but I wanted to highlight for everyone here that EAO is planning to put this up for Stage 3 in January

YSK: Question: is it strictly a promotion to 2.7?

SFC: It’s already at 2.7

YSK: Oh, I see! Do we already have some implementation of this proposal?

SFC: There’s implementation in the format.js polyfill that EAO made, but there is not an engine implementation, though there’s nothing stopping an implementation from implementing it now behind a flag. 

### Section 9.1 AvailableLocales shouldn't require base language if script is present #947

https://github.com/tc39/ecma402/issues/947

SFC: There’s discussion that’s already happened at the issue. The problem is that currently the spec requires the following: 

Additionally, for each element with more than one subtag, it must also include a less narrow language tag with the same language subtag and a strict subset of the same following subtags (i.e., omitting one or more) to serve as a potential fallback from ResolveLocale.
SFC: The reason the spec says this – this is confusing language – let’s say the spec supports zh-TW, it can’t support the language-region combo without supporting the reason, because the BasicMatcher strips off subtags until you get to the language subtag, and if any of the locales in the fallback chain are supported then the parent should be supported. The reason it might be a problem is that zh-TW [...]. This can also apply to Serbian – if it supports Serbian as in Macedonia it should not be necessarily required to support cyrillic serbian. This is not going to be a problem for ECMA-402 right now, until we can support dynamic loading of locales, that would allow additional languages to be adopted into this specific API. The question here is that they’re building this translation API where you can load languages one at a time into the API, and it would be a pain point if the zh subtag were to change meanings in the middle of a program running based on the order in which traditional and simplified chinese were added. HJS and RGN have both weighed in on this thread, and they’re in the queue.

HJS: For that comment, I was missing the context of the translation API that Dominic was proposing. My point is that if this is an embedded system that only has space for Hant, then they’ll do what they do, so now my question is, what kind of dependency does the translation API have with this spec requirement? Why is this spec requirement a constraint on the translation API? Why does ECMA-402 constrain the API?

SFC: I’ll try to answer that. Dominic was looking for prior art for performing the – using the translation API. If you try to create a, this translator thing, if you try to create an AI translator using that API and you give it a locale string to be like “here’s the source language, here’s the target language”, then what algorithm should the translation API use to select the correct model? Dominic was looking to use a preexisting fallback algorithm rather than trying to reinvent a different one for the translation API. One possible conclusion could be that the translation API problem space is so different from ours that we should say they should use their own fallback algorithm, rather than using ours. But there is value in them being able to just refer back to our fallback algorithm.

RGN: I think I’m not clear on what semantics you’re attaching to a couple terms. The shorter one: there are two fallback algorithms specified. There’s BasicMatcher and BestFitMatcher. Any external specification that’s referencing it should be clear on that difference. I suspect that in almost all scenarios the BestFit algorithm is the one to use, and it’s underspecified in 402 for precisely these kinds of reasons. Mapping zh-TW to zh-Hant uses data that comes from CLDR and is not algorithmic. There is no algorithm that specifies that, as I remember, I believe it’s data-driven. So when you talk about referring to “the fallback algorithm”, I’m curious as to what that means to you.

SFC: I’m not sure which choice Dominic is referring to, which of the fallback algorithms. I don’t think whether or not this proposal has a BestFt rather than a Basic locale matcher impacts the question of whether lower subtags need to be supported. The narrower subtags being supported is a spec requirement for 402. We could say that if using BestFitMatcher that requirement is relaxed.

RGN: This gets to the broader clarification question I have: what does it mean to “support” locale zh?

SFC: As far as the translation API is concerned, the locale is supported so long as there’s a model loaded that can translate into that language.

RGN: That also seems independent of the ECMA-402 text.  Let me pull up what we actually have. We don’t define support in this way, we just say that if it includes a narrow tag we also need the less narrow ones – in Lookup, not BestFt. If we want to talk about behavioral requirements I’m happy to do so, but I’m not happy to relax the text in a way that will result in the impossibility of implementing the lookup algorithm. 

HJS: Two points. If you ask for BestFit, giving Lookup is compliant. That’s a datapoint that to the extent that spidermonkey’s lookup works on the web, bestfit might be not as necessary as it looks. But to the point of “what does it mean to support something”, a key thing that needs to be understood that I don’t understand about this use case is how does this translation API decide to load something? Assume I’ve loaded a model that says I’m able to translate from German, and I’ve already downloaded a model that says I can target British English. If I ask for a translation from German to French, does that mean that it says “unsupported,” or does it say “hey, I know French is available over there,  I can download it”? Does matching run on what’s already downloaded, or some description of an available repository from which it can be fetched? If I ask for a translation from German to French and I have no French, and I have British English and I ask German to English, and in CLDR en means en-US? What’s the user experience that the translation API is after before ECMA-402 has to say anything about this? 

SFC: I think the issue here, just to reiterate, is let’s say that a website is using the zh translation model, and the zh translation model is loaded from Hant, because the Hans model has not yet been downloaded. The user goes and does something that causes the Hans model to be made available. Then the user comes back to the website that had been using the zh subtag, now that’s suddenly translated in Hans instead of Hant. I guess – ECMA-402 currently – if you’re using the smart matcher, BestFitMatcher, it requires everything in the fallback chain to be present. Therefore, if zh-Hant is present, then zh subtag should fail, since it is not loaded.

RGN: We don’t have a concept of failure at all. If I have no zh loaded into my runtime and I still try to localize into Chinese, I will get output. That is fundamental to the Intl APIs. 

SFC: Yes, it will fall back to the root translation, for the purpose of the translation API it will fall back to some root behavior. The translation API should maybe have an error case that can be followed.

RGN: If we’re missing a way to have them [something]. The process: when you enter zh-TW, or zh-Hant or zh-Hans, any of these, the smart algorithm will do the likely subtags logic. It will run that, strip off the unnecessary ones, it will provide output, you can query the system and see that zh-Hant and zh-Hans resolve differently. That’s fine. The point is to do the resolution and get your output, maybe require an exact match on whatever comes out, I don’t know for their purposes. If you were to further append subtags, which are okay to ignore and which are not? That seems to be a concern of the external consumer, not the consumer of 402. If I provided a further unicode subtag for region, I, as a developer, am aware of a regional difference, but the models that are loaded are not aware of that, then it is up to them – or me, in my input – how to resolve that. If you’re asking for Austrailian English, is it okay to provide U.S. English as a fallback? That’s not up to us.

HJS: If you request en, and you already have en-GB, and likely subtags for en is en-US, what mechanism causes an en-US download suggestion? And if that’s something the user controls – if the available locales list is the local one, and the user manages that – well, that’s pretty much the whole thing. And if on the other hand it’s “magic happens”, the server could just say “you could download these.” I’m still not clear on what’s available, if the available list is the local list, and if so what’s allowed to change it, and how does that change results? What are the constraints?

SFC: Let me rephrase this a little bit. I guess the situation is that – there could be a situation where an engine only supports a translation model for zh-Hant. No zh-Hans, even after download. In this environment, must it be a requirement that zh must resolve to zh-Hant, or should it fall back to root while zh-Hant and zh-TW have well-defined behavior?

FYT: Isn’t the fallback behavior of the locale outside our scope?

SFC: We already have that spec text

RGN: No, it says that availableLocales must contain the less narrow subtag (?). This doesn’t mean non-default. It can resolve to default data.

FYT: That’s different. I agree with RGN.

SFC: In the case of the translation API, resolving to default data means the locale isn’t supported at all. I was looking at their spec, and I think that if the locale is not in the list of supported locales, then an error of some sort appears to be returned. I can verify that. They don’t do the ECMA-402 behavior of using root data. 

RGN: It’s fine if they don’t

SFC: They reject the Promise, it looks like, which basically returns an error, so the translation API will, I’m 99% sure, return an error, which is different from what ECMAScript does, which is one thing that makes a translation API different here. I’m kind of reaching the conclusion that there’s sufficient differences between how the translation API works and how ECMA-402 works that I don’t think this particular ECMA-402 requirement, that if zh-TW is an available locale than zh should be as well, and it’s not clear that that requirement should translate over to the translation API, 1) because it doesn’t have anything it could put in the zh subtag, whereas in 402 we could put root data there, 2) because of this whole thing about dynamic loading of locales. Because they’re different, it doesn’t seem like the ECMA requirement should translate to the translation API

RGN: I agree, and further I think their use of the ECMA-402 algorithms seems reasonable. I think the algorithms we have support them, though one isn’t specified and the other is basically trivial. But their use of the [[AvailableLocales]] slots wouldn’t be appropriate, which goes back to HJS’s point –it’s going to be different, it’s dynamic in a way the 402 ones aren’t. Once you update the list, feed the new list into the lookup, it won’t be provided from a slot the way the 402 ones are. And the algorithms are perfectly content with [missed something]. AvailableLocales list contents are fed into the lookup algorithms, as any other list – I agree with you. I think that that particular constraint on the contents of the list does not apply for this translation API.

SFC: Let me give a deep link to the section of the translation API that I was looking at earlier. Looks like the spec text has changed a little since I last read it – they’ve moved things around – so it will take me a moment to find the exact paragraph referenced from the original issue. 

### Add kilowatt-hour unit to Intl.NumberFormat #739

https://github.com/tc39/ecma402/issues/739

SFC: We have a guest on the call, Kaleb who’s joining to talk about energy units in Intl.NumberFormat.

KAP: I don’t have a background in internationalization, but in our tools we’re often using energy units in our forecasting, and it would be useful to have it in the internationalization suite.
 
KAP: The units that are most important for us are watt, kilowatt, and kilowatt-hour. The use cases are listed below – the particular information the energy industry uses, the units most often found in both residential and industrial energy usage. One of the the main points for selecting these three is that they’re all in CLDR already, and so it seems like with a little bit of help this would be relatively easy for us to add and implement. That’s about all I have, I’m looking for any guidance here, and I apologize for being a little underprepared.

SFC: A little background here. We initially added units to the API about five years ago. There was feedback we got from engines, all this history is on github and if you click a few links back you can find it all, there was feedback from engines that shipping all CLDR units required so much data across all locales that it was increasing the download size of the browser. A browser that was 30MB would be 32MB, something like that. That was a cost that the browser engines wanted to reduce, and in order to do that we had to make a judgement call about what units to include – about 40 of them. The way we picked this is we looked at the unit preferences in CLDR, and included anything that some locale had as a preference. Something that’s happened since then is that we’ve been allowing folks such as yourself to go on the 402 repo and file issues for use cases we hadn’t considered. There’s currently 10 issues with this new unit label. One that’s gotten a lot of interest is adding binary prefix units, graphics units like pixels, nutrition units.

SFC: There’s two approaches that we as a group should explore. Note that these issues are filed by users, not implementers, which is good. Every other one of these is filed by a user, which is a good thing – people are asking for it. When people ask for it, it helps us prove motivation to add it. One approach this group could take is to just look at individual requests and move forward with specific units, or we could have something like Intl.NumberFormat new units proposal, and then go through all issues and pick out the ones that fit best in the proposal, and have that go through the stage process. We have some units with niche uses – “stone” as a measure of weight, for example – and kilowatt-hour seems more relevant than that. For these energy units there is increased interest compared to what we had in 2019, because of for example the increasing prevalence of electric cars.

SFC: Even within the energy units field there’s questions about what to include – if we have kilowatt, do we need megawatt?

KAP: Thanks for that background! To answer your question on what’s most important, if kilowatt-hour can be made up of components, that’s all we need. Kilowatt – storage/capacity, kilowatt-hour, measure of consumption. Being able to express both of those together is extremely important, found in both industrial and residential usage, higher capacities only useful for highly industrial or global measurements.

DMN: I think there’s a pretty good motivation for adding these. What we would have to do in terms of implementation is evaluating how large the data payload would be. In terms of having one large proposal with a bunch of requests, or smaller proposals, my preference would be for smaller proposals that would be less likely to get hung up in long discussions of which particular units are most useful. In terms of evaluating cost it would be easier to have a bunch of them at once, but for this one it makes sense to have it on its own.

FYT: I say similar things. First, the real issue in the past is really the size, and whenever we add a unit we have to think about different styles, and for each style all the locales need translations, so that’s bundled, so that’s a reason we don’t do that. I think this is well-motivated and support it, but I share DMN’s opinion re: going with the smaller proposals. Every time we go through this thing and people say the increase, so actually smaller incremental changes are easier to ship, even though process-wise it can take longer. It’s just so much easier that I’d prefer it, even if it means more process.

YSK: Thank you, I’m so happy that this request is coming from the community and not the implementers. I can see this is super motivated, and motivated everywhere. If this becomes this small-scoped one I would expect we could super quickly get this through super fast. I am very supportive of this proposal.

SFC: I agree that small proposals are easier to advance. I wanted to raise the question of evaluate them all at once because if we go the path of having several small proposals the end result is the same as if we had one big one, but with one big one we could see what the resulting impact is. But if the implementers are happy with small proposals with small impact, that is just fine. I agree that it is likely to be a smoother advancement through the stage process to have a narrow proposal. 

DMN: Well, the fact that we have someone willing to come to speak to the committee is a clear indicator of interest. Which is one reason why I think it makes sense to keep this one as a separate proposal. 

FYT: I do want to ask that we try to minimize the really needed one, so we could have a careful analysis of what’s needed. I say we should ship with the needed one, and if there’s things that you just think are nice to have, remove that.

KAP: Since size has come up a few times, is there a standard way that that’s measured before moving through with a proposal, so that we can attach it to the proposal?

FYT: I don’t think we need to count it, because we can find out from the past. It’s just that anything that’s not necessary, don’t include it. 

SFC: I don’t expect that – the way this goes is that adding any individual unit is likely to create a big enough size difference that it would make the proposal infeasible.

SFC: I think the next step would be to – I think we could bring this to stage 1 without much controversy. It’s possible that it could even advance directly to a later level, though given that this is new material we should go through the normal process. This proposal would then become an official TC39 proposal, then we’d write the spec text, which wouldn’t be too big – add a few things to the sanctioned items list, one of the easiest proposals you could write – but that would need to happen. And then it’s great, KAP, that there’s community members that are motivated enough to present these kinds of proposals. It’s kind of silly that having a community member join our call should be the bar, but it does filter out what things people really care about. And beside that, this does rise to the list of open issues based on number of upvotes. The sum of all these different signals is what we can use to say that this is a motivated proposal. The next steps are to put this on the agenda to be a stage 1 proposal. [recap of TC39 stage process]. Based on the comments today we could probably get stage 2 pretty quickly, but we do need spec text, so start with stage 1 and go to stage 2 at the january meeting. Members of the committee are also active on Matrix. 

FYT: A question: logistically, as a chair, what – if anyone wanted to put this on the next TC39 agenda, is there a limitation on who can present it? Does KAP have the role to do that?

SFC: If KAP were to present it himself he’d need approval to be an invited expert. He’s filled out those forms and they’re doing a process with it on the backend. It’s not necessary for KAP to present it, as long as the proposal is there and we have a slide presentation it’s not necessary for KAP to present it. 

FYT: I do want to point out that the – the time for us to put it on the agenda, we only have a couple of days.

SFC: I think we have enough material already to put it for stage 1. Stage 1 essentially just needs the README file. We don’t have enough for stage 2, though.

DMN: I don’t think supporting materials are needed pre-deadline for stage 0 / stage 1 presentations. It would be nice to have slides, but with the amount of detail you have in the readme that’s good as well. 

### Review TG1 Agenda

On agenda:

- Intl Locale Info for Stage 4
- Frank's PR 1032

Needs to be added:

- Ben's PR 1026 <- not needed, already has consensus
- Intl Keep Trailing Zeros update (Eemeli)
- Implementation-Defined Limits (Shane)
- Intl Energy Units for Stage 1 (Ben)
- Intl Era Month Code for Stage 3 (Ben) — needs email to TG2

### Potential topics for next month

Cyclic year doesn't show up in resolvedOptions #816
https://github.com/tc39/ecma402/issues/816

add String.toTitleCase, String.toLocaleTitleCase #294
https://github.com/tc39/ecma402/issues/294

Add a variant of ListFormat#formatToParts that returns the source objects #1004
https://github.com/tc39/ecma402/issues/1004
